%%% Intro.tex ---
%%
%% Filename: Intro.tex
%% Description:
%% Author: Ola Leifler
%% Maintainer:
%% Created: Thu Oct 14 12:54:47 2010 (CEST)
%% Version: $Id$
%% Version: 
%% Last-Updated: Thu May 19 14:12:31 2016 (+0200)
%% By: Ola Leifler
%% Update #: 5
%% URL:
%% Keywords:
%% Compatibility:
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%% Commentary:
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%% Change log:
%%
%%
%% RCS $Log$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%% Code:


\section{Introduction}
\label{cha:introduction}

\subsection{Motivation}
\label{sec:motivation}

In the domain of large-scale enterprise software development, the velocity of feature delivery is a critical competitive advantage. Modern Continuous Integration and Continuous Deployment (CI/CD) pipelines have enabled rapid iteration cycles, but they simultaneously impose significant demands on quality assurance processes \cite{graham2012experiences}. A critical bottleneck in this workflow is \textit{test scope analysis}, the activity of determining which legacy test cases are relevant for verification when a new feature, change request, or defect fix is introduced \cite{hemmati2013achieving}.

For large organizations, maintaining high test coverage while avoiding redundant execution is a complex optimization problem. When a new artifact, such as a user story or bug ticket, is introduced, test engineers must navigate vast repositories of test cases to identify existing coverage and potential gaps \cite{8054871}. Historically, this retrieval process has relied on keyword-based search or the manual inspection of static trace links. However, these traditional methods are increasingly brittle; they often fail to capture the semantic nuance of natural language requirements or the complex, graph-like dependencies between system components \cite{wang2022systematic, antoniol2025recovering}. As software systems grow in complexity, the "semantic gap" between unstructured change descriptions (e.g., "fix race condition in handover") and structured test cases (e.g., "TC\_HO\_001") widens, leading to inefficient scoping, missed defects, and increased maintenance costs \cite{fauzan2021different}.

Recent advancements in Large Language Models (LLMs), which are AI models capable of understanding and generating human-like text, and Retrieval-Augmented Generation (RAG), a technique that enhances LLMs by fetching relevant information from external data sources, offer promising avenues for bridging this semantic gap \cite{fan2023large, hou2024large}. Simultaneously, Knowledge Graphs (KGs), structured representations of interconnected entities and their relationships, provide a structured means to model the intricate relationships between requirements, code, and tests \cite{kesri2021autokg, radhakrishnan2023create}. By combining these technologies into an \textit{agentic architecture}, a system where autonomous AI agents orchestrate retrieval and reasoning, there is potential to transform test scope analysis from a manual, error-prone task into an automated, explainable, and highly accurate process \cite{fuchss2025lissa}.

The primary problem addressed in this thesis is the inefficiency and inaccuracy of current test scope analysis methods in large-scale software engineering contexts. Traditional keyword search approaches lack the semantic understanding required to match high-level requirements with low-level test steps, frequently resulting in low recall (missing relevant tests) or low precision (retrieving irrelevant tests).

While standard RAG approaches improve semantic matching, they often suffer from "hallucinations" or fail to account for the structural context inherent in software artifacts (e.g., inheritance, trace links, execution history) \cite{Wang2024SoftwareTesting}. Conversely, purely graph-based methods struggle to effectively process the unstructured free text found in change logs and user stories \cite{cheng2025survey}. Consequently, there is a lack of integrated solutions that leverage the combined strengths of \textit{vector-based semantic retrieval} and \textit{graph-based structural reasoning} to provide accurate and explainable test scope recommendations.

\subsection{Aim}
\label{sec:aim}

The aim of this thesis is to design, implement, and evaluate an agent-orchestrated RAG and Knowledge Graph architecture for test scope analysis. The study aims to determine the effectiveness of this hybrid approach in automating the retrieval of relevant legacy test cases, identifying coverage gaps, and providing explainable rationales for its recommendations, thereby increasing practical utility for software practitioners.

\subsection{Research questions}
\label{sec:research-questions}

To fulfill the aim, the following research questions have been formulated:

\begin{itemize}
    \item \textbf{RQ1:} How can a Knowledge Graph be modeled to effectively integrate heterogeneous legacy test artifacts (structured requirements vs. unstructured descriptions) for semantic retrieval?
    \item \textbf{RQ2:} To what extent does an agent-orchestrated GraphRAG approach improve retrieval accuracy (measured by precision@k and recall@k) compared to standard keyword-based and vector-only RAG methods?
    \item \textbf{RQ3:} How do software practitioners perceive the explainability and practical utility of the automated test scope recommendations provided by the system?
\end{itemize}

\subsection{Delimitations}
\label{sec:delimitations}

This study is delimited to the context of Ericssonâ€™s internal software development environment.
\begin{itemize}
    \item \textbf{Data Scope:} The study will utilize specific datasets provided by Ericsson, including structured test management data (requirements, trace links) and unstructured test case descriptions.
    \item \textbf{Evaluation Scope:} Performance evaluation is limited to offline metrics (precision, recall, MAP, nDCG) and a qualitative human-in-the-loop utility study with a selected group of Ericsson engineers.
    \item \textbf{Functional Scope:} The system focuses exclusively on \textit{Test Scope Analysis} (identifying relevant existing tests) and does not include the automatic generation of new test cases or the execution of tests.
\end{itemize}

\subsection{Disposition}
\label{sec:disposition}
The remainder of this report is structured as follows:
\begin{itemize}
    \item \textbf{Theoretical Framework:} Presents the relevant theory concerning LLMs, RAG, Knowledge Graphs, and software testing methodologies.
    \item \textbf{Method:} Describes the design science research methodology, the system architecture, and the evaluation metrics used.
    \item \textbf{Implementation:} Details the data ingestion process, Knowledge Graph schema design, and the implementation of the hybrid retrieval agents.
    \item \textbf{Results:} Presents the quantitative performance metrics from the comparative analysis and qualitative feedback from the practitioner user study.
    \item \textbf{Discussion:} Analyzes the results in relation to the research questions and related work, discussing the implications for the field.
    \item \textbf{Conclusion:} Summarizes the key findings and suggests directions for future research.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Intro.tex ends here


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: